{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "import sweetviz as sv\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "pd.set_option('display.max_columns', 70)\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data:\n",
    "3 different data is loaded in this analysis(\"ad06\" , \"defects\" and \"shipped+parcel\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading ad06 parquet file\n",
    "data_ad06 = pd.read_parquet('20211207_ad06_GATECH.parquet',engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ad06.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many columns are specified by steelcase to be droped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading ad06 brincl cols to drop or keep file\n",
    "xlsx = pd.ExcelFile(\"OneDrive_1_2-15-2022/ad06 brincl cols to drop or keep.xlsx\")\n",
    "drop_data = pd.read_excel(xlsx,\"ad06 cols to drop or keep\")\n",
    "\n",
    "#reading defects data\n",
    "defects = pd.read_excel(\"Defects.xlsx\")\n",
    "defects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reading Shipped+parcel data\n",
    "path = os.getcwd()\n",
    "files = os.listdir(path+\"/Pieces Shipped + Product Hierarchy 2-21-2022/2020\")\n",
    "y2020 = pd.DataFrame()\n",
    "for f in files:\n",
    "    data = pd.read_excel(\"Pieces Shipped + Product Hierarchy 2-21-2022/2020/\"+f)\n",
    "    y2020 = y2020.append(data)\n",
    "    \n",
    "    \n",
    "\n",
    "files = os.listdir(path+\"/Pieces Shipped + Product Hierarchy 2-21-2022/2021\")\n",
    "y2021 = pd.DataFrame()\n",
    "for f in files:\n",
    "    data = pd.read_excel(\"Pieces Shipped + Product Hierarchy 2-21-2022/2021/\"+f)\n",
    "    y2021 = y2021.append(data)\n",
    "    \n",
    "\n",
    "files = os.listdir(path+\"/Pieces Shipped + Product Hierarchy 2-21-2022/2022\")\n",
    "y2022 = pd.DataFrame()\n",
    "for f in files:\n",
    "    data = pd.read_excel(\"Pieces Shipped + Product Hierarchy 2-21-2022/2022/\"+f)\n",
    "    y2022 = y2022.append(data)\n",
    "\n",
    "data_connection = pd.concat([y2020,y2021,y2022])\n",
    "data_connection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping extra columns from ad06 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of drop columns \n",
    "drop_data[\"Drop?\"].fillna(\"NO\",inplace=True)\n",
    "drop_columns = drop_data[drop_data['Drop?'].str.contains(\"Yes\")][\"Field\"]\n",
    "\n",
    "#dropping columns from ad60\n",
    "data_ad06_clean = data_ad06.drop(list(drop_columns),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering defects data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter value stream = \"Components\" in defects data\n",
    "defects_filtered = defects[defects[\"Value Stream / Vendor\"]==\"Components\"]\n",
    "defects_filtered = defects_filtered.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defects_filtered['Cause Code Concat '].unique()\n",
    "# cause_code = ['1250 - Missing','1150 - Wrong Style','1151 - Wrong Finish']\n",
    "# defects_filtered = defects_filtered.loc[defects_filtered['Cause Code Concat '].isin(['1250 - Missing','1150 - Wrong Style','1151 - Wrong Finish'])]\n",
    "# defects_filtered = defects_filtered.reset_index(drop = True)\n",
    "# defects_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering shipped+parcel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter value stream = \"Components\" in shipped+parcel data\n",
    "data_connection_filtered = data_connection[data_connection[\"Value Stream\"]==\"Components\"]\n",
    "data_connection_filtered = data_connection_filtered.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ad06 data we need to change some columns format from str to int to be able to do merge with other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert some str column to int in ad06\n",
    "data_ad06_clean_int = data_ad06_clean[~data_ad06_clean[\"4ZSD_AD06_VBELN\"].str.contains('[A-Za-z]')]\n",
    "data_ad06_clean_int['4ZSD_AD06_VBELN'] = data_ad06_clean_int['4ZSD_AD06_VBELN'].map(lambda x:int(x))\n",
    "data_ad06_clean_int['4ZSD_AD06_POSNR'] = data_ad06_clean_int['4ZSD_AD06_POSNR'].map(lambda x:int(x))\n",
    "data_ad06_clean_int['4ZSD_AD06_UEPOS'] = data_ad06_clean_int['4ZSD_AD06_UEPOS'].map(lambda x:int(x))\n",
    "data_ad06_clean_int[\"concat\"] = data_ad06_clean_int[['4ZSD_AD06_VBELN','4ZSD_AD06_POSNR']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicate in shipped+parcel data since some orders were shipped in more than one part, we need to aggrefate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing date column from shipp data to group them based on other columns\n",
    "connection_col = list(data_connection_filtered)\n",
    "connection_col.remove(\"Act Ship Com Dt\")\n",
    "group_col = connection_col.copy()\n",
    "group_col.remove(\"Pieces Shipped\")\n",
    "data_connection_grouped = data_connection_filtered[connection_col].groupby(group_col).sum().reset_index()\n",
    "data_connection_grouped[\"concat\"] = data_connection_grouped[[\"Sales Order\",\"Sales Order item\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge \"ad06\" and \"shipped+parcel\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad06_connection = data_ad06_clean_int.merge(data_connection_grouped,on =[\"concat\"] )\n",
    "ad06_connection[\"style\"] = ad06_connection[[\"4ZSD_AD06_POSNR\",\"4ZSD_AD06_UEPOS\"]].apply(lambda x: x[\"4ZSD_AD06_UEPOS\"] if x[\"4ZSD_AD06_UEPOS\"]!=0 else x[\"4ZSD_AD06_POSNR\"],\n",
    "                                    axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_style = ad06_connection.copy()\n",
    "merge_style[\"claim\"] = \"NO\"\n",
    "for row in defects_filtered.iterrows():\n",
    "    merge_style.loc[(merge_style[\"4ZSD_AD06_VBELN\"] == row[1][\"Sales Order\"]) & (merge_style[\"style\"] == row[1][\"Sales Order item\"]),\"claim\"] = \"Yes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_style[\"claim\"].value_counts())\n",
    "print(merge_style[\"claim\"].value_counts()/len(merge_style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_drop_col = ['4ZSD_AD06_VBELN',\n",
    " '4ZSD_AD06_POSNR',\n",
    " '4ZSD_AD06_UEPOS',\n",
    " '0PLANT',\n",
    " '0PLANT___T',\n",
    " '4ZSD_AD06_MATNR',\n",
    " '4ZSD_AD06_KONDM', \n",
    " '4ZSD_AD06_ZZTUSPCLIND',\n",
    " 'GM Category',\n",
    " 'concat',\n",
    " 'Sales Order',\n",
    " 'Sales Order item',    \n",
    " 'Value Stream',\n",
    " 'Reporting Plant',\n",
    " 'Unnamed: 12',\n",
    " 'style',\n",
    " 'Material (Parcel)',\n",
    " '4ZSD_AD06_ERDAT',\n",
    " '4ZSD_AD06_ZZCONF_DELV',\n",
    " 'Product Sub-Cat',  \n",
    " '4ZSD_AD06_ZZCONF_MAD',\n",
    " '4ZSD_AD06_ZZCUSTREQDATE']\n",
    "\n",
    "merge_clean = merge_style.drop(list(merge_drop_col),axis=1)\n",
    "\n",
    "merge_clean_head = merge_clean.head()\n",
    "\n",
    "num_col = ['4ZSD_AD06_BRGEW',\n",
    " '4ZSD_AD06_KBMENG',\n",
    " '4ZSD_AD06_NTGEW',\n",
    " '4ZSD_AD06_VOLUM',\n",
    " '4ZSD_AD06_ZZLINEVOLUME',\n",
    " '4ZSD_AD06_ZZLINEWEIGHT',\n",
    " 'Pieces Shipped']\n",
    "\n",
    "cat_col = ['4ZSD_AD06_MATKL',\n",
    " '4ZSD_AD06_VSTEL',\n",
    " '4ZSD_AD06_ZZCARTIND',\n",
    " '4ZSD_AD06_ZZCONPATH',\n",
    " 'PCC',\n",
    " 'Product Line',\n",
    " 'Op Prod Category',\n",
    " 'PL Category',\n",
    " 'Work Center']\n",
    "\n",
    "\n",
    "\n",
    "for c in cat_col:\n",
    "    print('\\n------',c,'--------')\n",
    "    print(merge_clean[c].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_clean[\"4ZSD_AD06_ZZCARTIND\"].fillna(\"U\",inplace=True)\n",
    "\n",
    "# threshold = 3\n",
    "\n",
    "\n",
    "# for feature in cat_col:\n",
    "#     if len(merge_clean[feature].unique()) > 4:\n",
    "#         print(feature)\n",
    "#         index, counts = np.unique(merge_clean[feature].values,return_counts=True)\n",
    "#         df_temp = pd.DataFrame({feature:index,\"count\":counts})\n",
    "#         df_temp['percent'] = (df_temp['count'] / len(merge_clean)) * 100\n",
    "#         keep_ = df_temp.loc[df_temp['percent'] >= threshold , feature]\n",
    "#         merge_clean[feature] = merge_clean[feature].apply(lambda x:x if x in list(keep_) else \"others\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_preparation(object):\n",
    "    def __init__(self,data):\n",
    "        if len(data) == 0:\n",
    "            return('Data Is Empty !!!')\n",
    "        self.data = data\n",
    "        \n",
    "    def cat_aggregation(self, cols):\n",
    "        def condense_category(col, min_freq=0.05, new_name='other'):\n",
    "            series = pd.value_counts(col)\n",
    "            mask = (series/series.sum()).lt(min_freq)\n",
    "            return pd.Series(np.where(col.isin(series[mask].index), new_name, col))\n",
    "\n",
    "        for c in cols:\n",
    "            self.data[c] = condense_category(self.data[c], min_freq=0.05, new_name='other')\n",
    "            print('-------','Aggregated',c,'--------')\n",
    "            print(self.data[c].value_counts())\n",
    "            \n",
    "        return self.data   \n",
    "    \n",
    "              \n",
    "    def scaler(self, num_cols):\n",
    "        \n",
    "        # pipeline for numerical variables\n",
    "        # Filling missing values for numerical variable with the median of the data. Then, standardize the numerical variables.\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        num_var_fitted  = scaler.fit_transform(self.data[num_cols])\n",
    "        num_var_fitted = pd.DataFrame(num_var_fitted, columns=num_cols)\n",
    "        \n",
    "        data = self.data.drop(num_cols,axis = 1)\n",
    "        data = data.reset_index(drop = True)\n",
    "        num_var_fitted = num_var_fitted.reset_index(drop = True)\n",
    "        data = pd.concat([num_var_fitted, data], axis = 1)\n",
    "        return data, scaler\n",
    "    \n",
    "   \n",
    "    def OneHotEnc(self, cat_cols):\n",
    "        # Column Transformer based on the defined pipeline for numerical data and filled categorical data which are \n",
    "        # ready to be inputed to one-hot-encoder function.\n",
    "        for c in cat_cols:\n",
    "            self.data[c] = self.data[c].astype(str)\n",
    "        enc = OneHotEncoder()\n",
    "        enc_fit = enc.fit_transform(self.data[cat_cols])\n",
    "        col_names = enc.get_feature_names(cat_cols)\n",
    "\n",
    "        enc_fit = enc_fit.toarray()\n",
    "        enc_fit = pd.DataFrame(data = enc_fit, columns = col_names)\n",
    "        \n",
    "        data = self.data.drop(cat_cols,axis = 1)\n",
    "        data = data.reset_index(drop = True)\n",
    "        enc_fit = enc_fit.reset_index(drop = True)\n",
    "        \n",
    "        data = pd.concat([data,enc_fit], axis = 1)\n",
    "        \n",
    "        return data, col_names\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_clean = data_preparation(data = merge_clean).cat_aggregation(cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_clean, cat_names = data_preparation(data = merge_clean).OneHotEnc(cat_col)\n",
    "merge_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# down sampleing\n",
    "claim = merge_clean[merge_clean[\"claim\"] == \"Yes\"]\n",
    "non_claim = merge_clean[merge_clean[\"claim\"] == \"NO\"]\n",
    "non_claim_downsample = resample(non_claim,\n",
    "                         replace=True,\n",
    "                         n_samples=len(claim),\n",
    "                         random_state=42)\n",
    "merge_clean_downsample = pd.concat([claim, non_claim_downsample]).reset_index(drop=True)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(merge_clean_downsample, merge_clean_downsample[\"claim\"]):\n",
    "    ds_train_set = merge_clean_downsample.loc[train_index]\n",
    "    ds_test_set = merge_clean_downsample.loc[test_index]\n",
    "\n",
    "\n",
    "#check the ratio\n",
    "print(ds_test_set[\"claim\"].value_counts() / len(ds_test_set))\n",
    "print(merge_clean_downsample[\"claim\"].value_counts() / len(merge_clean_downsample))\n",
    "\n",
    "\n",
    "ds_train_x = ds_train_set.loc[:, ds_train_set.columns != 'claim']\n",
    "ds_train_y = ds_train_set[\"claim\"]\n",
    "ds_train_y[ds_train_y == \"NO\"] = 0\n",
    "ds_train_y[ds_train_y == \"Yes\"] = 1\n",
    "y_train = ds_train_y.astype('int')\n",
    "\n",
    "ds_test_x = ds_test_set.loc[:, ds_test_set.columns != 'claim']\n",
    "ds_test_y = ds_test_set[\"claim\"]\n",
    "ds_test_y[ds_test_y == \"NO\"] = 0\n",
    "ds_test_y[ds_test_y == \"Yes\"] = 1\n",
    "y_test = ds_test_y.astype('int')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merge_clean_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, scaler = data_preparation(data = ds_train_x).scaler(num_col)\n",
    "X_train.head()\n",
    "# list(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(ds_test_x[num_col])\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=num_col)\n",
    "X_test = ds_test_x.reset_index(drop = True)\n",
    "X_test = pd.concat([X_test_scaled, X_test[cat_names]], axis = 1)\n",
    "X_test.head()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat_target = X_train.copy()\n",
    "X_train_feat_target['claim'] = y_train\n",
    "cor_matrix = X_train_feat_target.corr()\n",
    "cor_matrix['claim'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1-based Linear SVM feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "df_prepared_reduced_l1_Based = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the reduced data\n",
    "print('number of selected variables:', df_prepared_reduced_l1_Based.shape[1])\n",
    "feature_idx = model.get_support()\n",
    "feature_name = X.columns[feature_idx]\n",
    "print('\\nname of selected variables:', feature_name)\n",
    "l1_based_features = list(feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lasso = linear_model.Lasso(alpha = 0.01).fit(X, y)\n",
    "print('number of selected variables:', len(X.columns[lasso.coef_ > 0]))\n",
    "print('\\nname of selected variables:', X.columns[lasso.coef_ > 0])\n",
    "lasso_based_features = list(X.columns[lasso.coef_ > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "df_prepared_reduced_Tree_Based = model.transform(X)\n",
    "df_prepared_reduced_Tree_Based.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_ #array with importances of each feature\n",
    "features_to_keep = X.columns[importances > np.mean(importances)]\n",
    "tree_based_features = list(features_to_keep)\n",
    "print(features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_dict = {'l1':l1_based_features,\n",
    "                          'lasso':lasso_based_features,\n",
    "                          'tree':tree_based_features\n",
    "                         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection/Setup AND Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import json\n",
    "def expand_grid(dictionary):\n",
    "    return pd.DataFrame([row for row in product(*dictionary.values())], \n",
    "                       columns=dictionary.keys())\n",
    "\n",
    "def create_model_library():\n",
    "    # Heavy version\n",
    "#     hyper_dict = {'KNN':{'n_neighbors':[3,5,7,9]},\n",
    "#                'SVM':{'C':[0.5, 1, 5, 10],\n",
    "#                  'gamma':[0.001,0.01,0.1,1,10]\n",
    "#                 },\n",
    "#                'RF':{'n_estimators':[20,40,60,100,200,300,400]},\n",
    "#                'XGboost':{'n_estimators':[40,80,150,200,300],\n",
    "#                      'max_depth':[3,4,5,6,8,10]\n",
    "#                     },\n",
    "#                'DL':{'batch_size':[8,16,32,64,128],\n",
    "#                 'hidden_layer_sizes':[1,2,3,4]\n",
    "#                },\n",
    "#                'Adaboost':{'n_estimators':[10,40,80,150,200,300]},\n",
    "#                'NaiveBayes':{'alpha':[0.001,0.01,0.1,1]},\n",
    "#                'logisticReg':{'C':[0.001,0.01,0.1,0.25]}}\n",
    "    \n",
    "#     # light version\n",
    "#     hyper_dict = {'KNN':{'n_neighbors':[3,5]},\n",
    "#                'SVM':{'C':[0.5, 1],\n",
    "#                  'gamma':[0.001,0.01,10]\n",
    "#                 },\n",
    "#                'RF':{'n_estimators':[20,40,60,100,200,300,400]},\n",
    "#                'XGboost':{'n_estimators':[40,80,150],\n",
    "#                      'max_depth':[3,4,5]\n",
    "#                     },\n",
    "#                'DL':{'batch_size':[32],\n",
    "#                 'hidden_layer_sizes':[1,2,3,4]\n",
    "#                },\n",
    "#                'Adaboost':{'n_estimators':[10,40,80]},\n",
    "#                'NaiveBayes':{'alpha':[0.001,1]},\n",
    "#                'logisticReg':{'C':[0.1,0.25]}}\n",
    "    \n",
    "    # test version\n",
    "    \"\"\"\n",
    "                   'SVM':{'C':[0.5, 1],\n",
    "                 'gamma':[0.01,10]\n",
    "                },\n",
    "    \"\"\"\n",
    "    # first step is defing a dictionary of the selected models with their hyper parameters. \n",
    "    hyper_dict = {'KNN':{'n_neighbors':[3,5]},\n",
    "               'RF':{'n_estimators':[20,100,50,200]},\n",
    "               'XGboost':{'n_estimators':[40,150,300],\n",
    "                     'max_depth':[3,4]\n",
    "                    },\n",
    "                'DL':{'batch_size':[32,64],\n",
    "                 'hidden_layer_sizes':[1,2,3,4]\n",
    "                },\n",
    "               'Adaboost':{'n_estimators':[10,80,50,100]},\n",
    "               'NaiveBayes':{'alpha':[0.001,1]},\n",
    "               'logisticReg':{'C':[0.25,0.5]}}\n",
    "    \n",
    "    # Three varible selection method have been applied to select the feutures with important prediction power.\n",
    "    variable_selection = ['l1', 'lasso', 'tree']\n",
    "    \n",
    "    # Creat the output of this function, which is a Pandas DataFrame.\n",
    "    model_library = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for model_name in hyper_dict.keys(): # for each of the classification models\n",
    "        temp = pd.DataFrame() # create a DataFrame which later will be concatinated to the model_library\n",
    "        params = hyper_dict[model_name] # find the defined hyperparameter for that specific model\n",
    "        \n",
    "        # 'expand_grid' expands each of the hyperparameters.\n",
    "        #(If 3 values are defined for a hyperparameter, three rows will be added to the model_library)\n",
    "        temp['parameters'] = expand_grid(params).to_dict(orient='records') \n",
    "\n",
    "        temp['method_id'] = model_name\n",
    "        model_wise_library = pd.DataFrame()\n",
    "        for var_sel in variable_selection: # For each of the variable selection approaches, define a unique scenario. \n",
    "            temp['variable_selection_method'] = var_sel\n",
    "            model_wise_library = pd.concat([model_wise_library, temp]) # Each of the scenarios will be concatinated.\n",
    "            \n",
    "        # For each classification model, obtained scenarios will be concatinated to model_library.\n",
    "        model_library = pd.concat([model_wise_library, model_library]) \n",
    "    model_library['method_param_id'] = list(range(1, model_library.shape[0]+1)) # A unique ID for each scenario. \n",
    "    model_library = model_library.reset_index(drop=True)\n",
    "    return model_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_library = create_model_library()\n",
    "model_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_obj(model_lib, n_features):\n",
    "    # based on the 'method_id', creates the method_name\n",
    "    method_name = model_lib['method_id'].values[0]\n",
    "    \n",
    "    # define the hyperparameters for that specific scenario, which is inputed to this fucntion. It stores like a dictionary.\n",
    "    params = model_lib.parameters.tolist()[0]\n",
    "    params = json.loads(str(params).replace(\"'\", '\"'))\n",
    "\n",
    "\n",
    "    if method_name=='KNN':\n",
    "\n",
    "        model = KNeighborsClassifier(n_neighbors = params['n_neighbors'])\n",
    "\n",
    "    elif method_name=='SVM':\n",
    "\n",
    "        model = SVC(C = params['C'], gamma = params['gamma'], probability = True)\n",
    "\n",
    "    elif method_name == 'RF':\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators = params['n_estimators'])\n",
    "\n",
    "    elif method_name == \"XGboost\":\n",
    "\n",
    "        model = GradientBoostingClassifier(learning_rate = 0.01,\n",
    "                                                n_estimators = params['n_estimators'],\n",
    "                                                max_depth = params['max_depth'])\n",
    "\n",
    "    elif method_name == \"DL\":\n",
    "\n",
    "        number_features = n_features\n",
    "\n",
    "        hidden_layer_sizes = params['hidden_layer_sizes']\n",
    "\n",
    "        if(hidden_layer_sizes == 1):\n",
    "\n",
    "            hidden_layer = (max(int(number_features/2), 1), max(int(number_features/4),1))\n",
    "\n",
    "        elif(hidden_layer_sizes == 2):\n",
    "\n",
    "            hidden_layer = (max(int(number_features),1), max(int(number_features/2),1))\n",
    "\n",
    "        elif(hidden_layer_sizes == 3):\n",
    "\n",
    "            hidden_layer = (max(int(number_features/2),1), max(int(number_features/3),1), max(int(number_features/6),1))\n",
    "\n",
    "        elif(hidden_layer_sizes == 4):\n",
    "\n",
    "            hidden_layer = (max(int(number_features),1), max(int(number_features/2),1), max(int(number_features/4),1))\n",
    "\n",
    "        model = MLPClassifier(hidden_layer_sizes = hidden_layer,\n",
    "                                   batch_size = params['batch_size'], \n",
    "                                  max_iter = 400)\n",
    "\n",
    "    elif method_name == \"Adaboost\":\n",
    "\n",
    "        model = AdaBoostClassifier(n_estimators = params['n_estimators'], learning_rate = 0.01)\n",
    "\n",
    "    elif method_name == \"NaiveBayes\":\n",
    "\n",
    "        model = BernoulliNB(alpha = params['alpha'])\n",
    "\n",
    "    elif method_name == \"logisticReg\":\n",
    "\n",
    "        model =  LogisticRegression(C = params['C'])\n",
    "\n",
    "    else:\n",
    "\n",
    "        model = None \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_computation(method_param_id,\n",
    "                             model_library, \n",
    "                             selected_features_dict, \n",
    "                             X_train, \n",
    "                             Y_train,\n",
    "                             X_test,\n",
    "                             Y_test\n",
    "                            ):\n",
    "    # select a specific scenario defined as a classification model with unique hyperparameters and selected features. \n",
    "    method_wise_lib = model_library.loc[model_library.method_param_id == method_param_id, ]\n",
    "    # retrieve the future selection method, among available three approaches.\n",
    "    var_sel_method = method_wise_lib.variable_selection_method.tolist()[0]\n",
    "    # select the feature selection approach based on the designed dictionary of the three available approaches.\n",
    "    imp_vars = selected_features_dict[var_sel_method]\n",
    "    # creat the actual model based on the classification model, hyperparameter value, feature selection approach (stored in method_wise_lib)\n",
    "    # and number of features.\n",
    "    model = create_model_obj(method_wise_lib, len(imp_vars))\n",
    "    # store the data for selected features\n",
    "    X_train_with_selected_vars = X_train[imp_vars]\n",
    "    # fit the model\n",
    "    model.fit(X_train_with_selected_vars, Y_train)\n",
    "    # since AUC is the evaluation criteria, probability will be the output of the model.\n",
    "    prob = model.predict_proba(X_test[imp_vars])\n",
    "    score = roc_auc_score(Y_test, prob[:, 1])\n",
    "    temp_output = {'method_param_id':[method_param_id],\n",
    "                   'roc_auc_score':[score]\n",
    "                  }\n",
    "    temp_output = pd.DataFrame(temp_output)\n",
    "    return temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel computation libraries\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# keep in mind that 'method_param_id' is not set here since the paralleization is based on 'method_param_id'\n",
    "partial_parallel = partial(run_parallel_computation,\n",
    "                           model_library = model_library, \n",
    "                           selected_features_dict=selected_features_dict, \n",
    "                           X_train=X_train, \n",
    "                           Y_train=y_train,\n",
    "                           X_test=X_test,\n",
    "                           Y_test=y_test)\n",
    "\n",
    "output = Parallel(n_jobs=num_cores)(delayed(partial_parallel)(i) for i in model_library.method_param_id.tolist())\n",
    "results = pd.concat(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_result = results.sort_values(by = \"roc_auc_score\",ascending = False)\n",
    "final_result = sorted_result.merge(model_library, on = \"method_param_id\",how = \"inner\")\n",
    "final_result.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = GradientBoostingClassifier(learning_rate = 0.01,\n",
    "                                         n_estimators = 300,\n",
    "                                         max_depth = 4)\n",
    "imp_vars = selected_features_dict[\"l1\"]                                               \n",
    "X_train_with_selected_vars = X_train[imp_vars]\n",
    "best_model.fit(X_train_with_selected_vars, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "print(best_model.feature_importances_)\n",
    "print(imp_vars)\n",
    "pyplot.bar(imp_vars, best_model.feature_importances_)\n",
    "pyplot.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "print(best_model.feature_importances_)\n",
    "print(imp_vars)\n",
    "pyplot.bar(imp_vars, best_model.feature_importances_)\n",
    "pyplot.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box( cols, col_x = 'claim'):\n",
    "    for col in cols:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.boxplot(x = col_x, y = float(col), data = merge_clean_downsample)\n",
    "        plt.xlabel(col_x) # Set label for the x axis\n",
    "        plt.ylabel(col) # Set label for y axis\n",
    "        plt.show()\n",
    "plot_box(cols = [\"4ZSD_AD06_ZZLINEVOLUME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "tmp_df = merge_clean_downsample[[\"4ZSD_AD06_ZZLINEVOLUME\",\"claim\"]]\n",
    "tmp_df['4ZSD_AD06_ZZLINEVOLUME'] = pd.to_numeric(tmp_df['4ZSD_AD06_ZZLINEVOLUME'])\n",
    "#tmp_df = tmp_df.loc[tmp_df['4ZSD_AD06_ZZLINEVOLUME']<200, ]\n",
    "sns.boxplot(x = 'claim', y = '4ZSD_AD06_ZZLINEVOLUME', data = tmp_df)\n",
    "plt.xlabel('claim') # Set label for the x axis\n",
    "plt.ylabel('4ZSD_AD06_ZZLINEVOLUME') # Set label for y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.plot.scatter(x = 'claim', y = '4ZSD_AD06_ZZLINEVOLUME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_center_data = merge_style[merge_style[\"claim\"]==\"Yes\"]\n",
    "work_center = (work_center_data[\"Work Center\"].value_counts()/merge_style[\"Work Center\"].value_counts())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Work Center\", data=work_center_data)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Work Center\", data=merge_style)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_center.plot.bar()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
